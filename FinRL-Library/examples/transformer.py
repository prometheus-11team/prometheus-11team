import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import os
import pickle

# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
def load_and_process_data():
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    
    # 1. ì£¼ê°€ ë° ê¸°ìˆ ì§€í‘œ ë°ì´í„°
    momentum_data_url = '/Users/gamjawon/FinRL-Library/examples/data/M7_stock_data_with_indicators.csv'
    momentum_data = pd.read_csv(momentum_data_url)
    momentum_data = momentum_data[['date', 'ticker', 'close', 'ROC_10', 'RSI_14', 'MACD']]
    momentum_data['date'] = pd.to_datetime(momentum_data['date'])
    momentum_data = momentum_data[momentum_data['date'] >= '2020-01-01'].reset_index(drop=True)
    print(f"ì£¼ê°€ ë°ì´í„°: {momentum_data.shape}")
    print(f"ì£¼ê°€ ë°ì´í„° ticker ì¢…ë¥˜: {momentum_data['ticker'].unique()}")
    
    # 2. ê±°ì‹œê²½ì œì§€í‘œ ë°ì´í„°
    macro_data_url = '/Users/gamjawon/FinRL-Library/examples/data/macro_indicators_2020_2024.csv'
    macro_data = pd.read_csv(macro_data_url)
    macro_data.rename(columns={'Unnamed: 0': 'date'}, inplace=True)
    macro_data['date'] = pd.to_datetime(macro_data['date'])
    
    # ê±°ì‹œê²½ì œì§€í‘œ ì¼ê°„ ë°ì´í„°ë¡œ ë³€í™˜
    macro_data.set_index('date', inplace=True)
    daily_index = pd.date_range(start='2020-01-01', end='2025-04-30', freq='D')
    macro_data_daily = macro_data.reindex(daily_index, method='ffill')
    macro_data_daily = macro_data_daily.reset_index().rename(columns={'index': 'date'})
    print(f"ê±°ì‹œê²½ì œ ë°ì´í„°: {macro_data_daily.shape}")
    
    # 3. ì¬ë¬´ì œí‘œ ë°ì´í„°
    financial_data_url = '/Users/gamjawon/FinRL-Library/examples/data/M7_financial_data_2020_2025.csv'
    financial_data = pd.read_csv(financial_data_url)
    print(f"ì¬ë¬´ ë°ì´í„° ì›ë³¸: {financial_data.shape}")
    print(f"ì¬ë¬´ ë°ì´í„° ì»¬ëŸ¼: {financial_data.columns.tolist()}")
    print(f"ì¬ë¬´ ë°ì´í„° ìƒ˜í”Œ:\n{financial_data.head()}")
    
    # íšŒì‚¬ëª…ê³¼ Ticker ë§¤í•‘
    company_to_ticker = {
        'Alphabet': 'GOOGL',
        'Amazon': 'AMZN',
        'Apple': 'AAPL',
        'Meta': 'META',
        'Microsoft': 'MSFT',
        'Nvidia': 'NVDA',
        'Tesla': 'TSLA'
    }
    
    financial_data['ticker'] = financial_data['Company'].map(company_to_ticker)
    financial_data['date'] = pd.to_datetime(financial_data['Release Date'])
    print(f"ì¬ë¬´ ë°ì´í„° ë§¤í•‘ í›„ ticker: {financial_data['ticker'].unique()}")
    print(f"ì¬ë¬´ ë°ì´í„° ë‚ ì§œ ë²”ìœ„: {financial_data['date'].min()} ~ {financial_data['date'].max()}")
    
    # í•„ìš”í•œ ì¬ë¬´ í”¼ì²˜ ì„ íƒ
    features = ['Operating Income', 'Net Income', 'EPS Diluted', 'Total Assets', 'Shareholders Equity']
    
    # ì¬ë¬´ë°ì´í„° ì¼ê°„ ë³€í™˜
    daily_financial_list = []
    for ticker in financial_data['ticker'].unique():
        if pd.isna(ticker):  # NaN ticker ìŠ¤í‚µ
            continue
            
        df_t = (
            financial_data
            .loc[financial_data['ticker'] == ticker, ['date', 'ticker'] + features]
            .copy()
            .set_index('date')
            .sort_index()
        )
        print(f"{ticker} ì¬ë¬´ ë°ì´í„°: {df_t.shape}")
        
        if len(df_t) == 0:  # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            print(f"{ticker} ì¬ë¬´ ë°ì´í„° ì—†ìŒ")
            continue
            
        df_t = df_t[~df_t.index.duplicated(keep='last')]
        df_t = df_t.reindex(daily_index).ffill().infer_objects(copy=False)
        df_t['ticker'] = ticker
        df_t = df_t.reset_index().rename(columns={'index': 'date'})
        daily_financial_list.append(df_t)
    
    if daily_financial_list:
        daily_financial_data = pd.concat(daily_financial_list, ignore_index=True)
        print(f"ì¼ê°„ ì¬ë¬´ ë°ì´í„°: {daily_financial_data.shape}")
        print(f"ì¼ê°„ ì¬ë¬´ ë°ì´í„° NaN ê°œìˆ˜:\n{daily_financial_data[features].isna().sum()}")
    else:
        print("âŒ ì¬ë¬´ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨")
        # ì¬ë¬´ ë°ì´í„° ì—†ì´ ì§„í–‰
        daily_financial_data = None
    
    # 4. ë°ì´í„° ë³‘í•©
    # ì£¼ê°€ + ê±°ì‹œê²½ì œ (ì¬ë¬´ë°ì´í„°ëŠ” ëª¨ë“  ê°’ì´ NaNì´ë¯€ë¡œ ì œì™¸)
    full_merged = pd.merge(momentum_data, macro_data_daily, on='date', how='left')
    print(f"ìµœì¢… ë³‘í•© ë°ì´í„°: {full_merged.shape}")
    
    return full_merged

def prepare_training_data(df):
    """í•™ìŠµ ë°ì´í„° ì¤€ë¹„"""
    
    print(f"ì´ˆê¸° ë°ì´í„° í˜•íƒœ: {df.shape}")
    print(f"ì»¬ëŸ¼ëª…: {df.columns.tolist()}")
    
    # íƒ€ê²Ÿ ìƒì„±: ë‹¤ìŒ 5ì¼ ìˆ˜ìµë¥  > 0 â†’ 1, else 0
    df['date'] = pd.to_datetime(df['date'])
    print(f"ë‚ ì§œ ë³€í™˜ í›„: {df.shape}")
    
    # í‹°ì»¤ë³„ë¡œ ì‹œí”„íŠ¸ ì ìš©
    df = df.sort_values(['ticker', 'date']).reset_index(drop=True)
    df['Signal'] = df.groupby('ticker')['close'].transform(
        lambda x: ((x.shift(-5) / x) - 1 > 0).astype(int)
    )
    print(f"Signal ìƒì„± í›„: {df.shape}")
    print(f"Signal ê°’ ë¶„í¬:\n{df['Signal'].value_counts()}")
    
    # NaNì´ ë§ì€ ì»¬ëŸ¼ ì œê±° (ì„ê³„ê°’: 50% ì´ìƒ NaN)
    nan_threshold = 0.5
    nan_ratio = df.isna().sum() / len(df)
    high_nan_cols = nan_ratio[nan_ratio > nan_threshold].index.tolist()
    
    if high_nan_cols:
        print(f"NaNì´ ë§ì€ ì»¬ëŸ¼ ì œê±°: {high_nan_cols}")
        df = df.drop(columns=high_nan_cols)
    
    # Signal NaN ì œê±°
    print(f"NaN ê°œìˆ˜ (Signal): {df['Signal'].isna().sum()}")
    df = df.dropna(subset=['Signal']).reset_index(drop=True)
    print(f"Signal NaN ì œê±° í›„: {df.shape}")
    
    # ë‚˜ë¨¸ì§€ NaN ê°’ë“¤ì„ 0ìœ¼ë¡œ ì±„ìš°ê¸° (ë˜ëŠ” í‰ê· ê°’ìœ¼ë¡œ)
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if col not in ['date', 'Signal']:
            df[col] = df[col].fillna(df[col].mean())
    
    print(f"NaN ì²˜ë¦¬ í›„ ë°ì´í„° í˜•íƒœ: {df.shape}")
    print(f"ë‚¨ì€ NaN ê°œìˆ˜:\n{df.isna().sum().sum()}")
    
    if len(df) == 0:
        print("âŒ ëª¨ë“  ë°ì´í„°ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤!")
        return None, None, None, None
    
    # ì‹œê³„ì—´ ë¶„í• 
    split_date = '2024-01-01'
    train_df = df[df['date'] < split_date].copy().reset_index(drop=True)
    test_df = df[df['date'] >= split_date].copy().reset_index(drop=True)
    
    print(f"í•™ìŠµ ë°ì´í„°: {train_df.shape}")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_df.shape}")
    
    if len(train_df) == 0:
        print("âŒ í•™ìŠµ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
        return None, None, None, None
    
    # í”¼ì²˜ ì»¬ëŸ¼ ì„ íƒ
    feature_cols = [c for c in train_df.columns 
                   if c not in ['date', 'ticker', 'close', 'Signal']]
    
    print(f"í”¼ì²˜ ì»¬ëŸ¼: {feature_cols}")
    print(f"í”¼ì²˜ ë°ì´í„° í˜•íƒœ: {train_df[feature_cols].shape}")
    
    # ì •ê·œí™”
    scaler = StandardScaler()
    train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])
    if len(test_df) > 0:
        test_df[feature_cols] = scaler.transform(test_df[feature_cols])
    
    return train_df, test_df, feature_cols, scaler

def create_sequence(df, feature_cols, window=30):
    """ì‹œê³„ì—´ ìœˆë„ìš° ìƒì„±"""
    X, y = [], []
    vals = df[feature_cols].values
    sigs = df['Signal'].values
    
    for i in range(len(df) - window):
        X.append(vals[i:i+window])
        y.append(sigs[i+window])
    
    return np.array(X), np.array(y)

class TransformerClassifier(nn.Module):
    """Transformer ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸"""
    
    def __init__(self, input_dim, d_model=64, nhead=4, layers=2):
        super().__init__()
        self.input_dim = input_dim
        self.d_model = d_model
        self.nhead = nhead
        self.layers = layers
        
        self.embed = nn.Linear(input_dim, d_model)
        enc = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True)
        self.encoder = nn.TransformerEncoder(enc, layers)
        self.head = nn.Linear(d_model, 1)
        
        # ì˜ˆì¸¡ í™•ë¥  ë©”ì„œë“œ ì¶”ê°€
        self.predict_proba = self._predict_proba
    
    def forward(self, x):
        x = self.embed(x)
        x = self.encoder(x)
        return torch.sigmoid(self.head(x[:, -1]))
    
    def _predict_proba(self, x):
        """ì˜ˆì¸¡ í™•ë¥  ë°˜í™˜ (2í´ë˜ìŠ¤ ë¶„ë¥˜ìš©)"""
        with torch.no_grad():
            self.eval()
            prob_positive = self.forward(x).cpu().numpy()
            prob_negative = 1 - prob_positive
            return np.column_stack([prob_negative, prob_positive])

def train_model(X_train, y_train, epochs=50, batch_size=32, lr=1e-3, val_ratio=0.2):
    """ëª¨ë¸ í•™ìŠµ"""
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Training on device: {device}")
    
    # ë°ì´í„° ì¤€ë¹„
    X_t = torch.tensor(X_train, dtype=torch.float32)
    y_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
    ds = TensorDataset(X_t, y_t)
    
    # Train/Val ë¶„í• 
    n_val = int(len(ds) * val_ratio)
    n_trn = len(ds) - n_val
    trn_ds, val_ds = random_split(ds, [n_trn, n_val])
    
    trn_loader = DataLoader(trn_ds, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = TransformerClassifier(input_dim=X_train.shape[2]).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.BCELoss()
    
    # í•™ìŠµ ë£¨í”„
    best_val_loss = float('inf')
    best_model = None
    
    for epoch in range(1, epochs + 1):
        # Training
        model.train()
        trn_losses = []
        for xb, yb in trn_loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            preds = model(xb)
            loss = loss_fn(preds, yb)
            loss.backward()
            optimizer.step()
            trn_losses.append(loss.item())
        
        # Validation
        model.eval()
        val_losses = []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(device), yb.to(device)
                preds = model(xb)
                loss = loss_fn(preds, yb)
                val_losses.append(loss.item())
        
        avg_trn_loss = np.mean(trn_losses)
        avg_val_loss = np.mean(val_losses)
        
        # ìµœì  ëª¨ë¸ ì €ì¥ (ì „ì²´ ëª¨ë¸ ê°ì²´ ë³µì‚¬)
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            # ëª¨ë¸ ì „ì²´ë¥¼ CPUë¡œ ë³µì‚¬
            best_model = TransformerClassifier(input_dim=X_train.shape[2])
            best_model.load_state_dict(model.state_dict())
            best_model.eval()
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch:02d} | Train Loss: {avg_trn_loss:.4f} | Val Loss: {avg_val_loss:.4f}")
    
    return best_model

def save_model_and_scaler(model, scaler, feature_cols, save_dir='./models'):
    """ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (ì™„ì „í•œ ê°ì²´ í˜•íƒœë¡œ)"""
    
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    # 1. ì™„ì „í•œ ëª¨ë¸ ê°ì²´ ì €ì¥
    model_path = os.path.join(save_dir, 'transformer_classifier.pt')
    torch.save(model, model_path)  # ì™„ì „í•œ ê°ì²´ ì €ì¥
    print(f"âœ… ì™„ì „í•œ ëª¨ë¸ ê°ì²´ ì €ì¥: {model_path}")
    
    # 2. ìŠ¤ì¼€ì¼ëŸ¬ì™€ í”¼ì²˜ ì •ë³´ ì €ì¥
    scaler_path = os.path.join(save_dir, 'scaler_and_features.pkl')
    with open(scaler_path, 'wb') as f:
        pickle.dump({
            'scaler': scaler,
            'feature_cols': feature_cols
        }, f)
    print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {scaler_path}")
    
    # 3. ì €ì¥ëœ ëª¨ë¸ ê²€ì¦ (PyTorch 2.6+ í˜¸í™˜)
    try:
        loaded_model = torch.load(model_path, map_location='cpu', weights_only=False)
        if hasattr(loaded_model, 'eval'):
            print("âœ… ëª¨ë¸ ì €ì¥ ê²€ì¦ ì™„ë£Œ - ì™„ì „í•œ ê°ì²´ í˜•íƒœ")
            print(f"ëª¨ë¸ íƒ€ì…: {type(loaded_model)}")
            print(f"ëª¨ë¸ êµ¬ì¡°: {loaded_model}")
        else:
            print("âŒ ëª¨ë¸ ì €ì¥ ê²€ì¦ ì‹¤íŒ¨ - ì™„ì „í•œ ê°ì²´ê°€ ì•„ë‹˜")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì €ì¥ ê²€ì¦ ì‹¤íŒ¨: {e}")

def load_trained_model(model_path, scaler_path):
    """í•™ìŠµëœ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
    
    try:
        # 1. ì™„ì „í•œ ëª¨ë¸ ê°ì²´ ë¡œë“œ (PyTorch 2.6+ í˜¸í™˜)
        model = torch.load(model_path, map_location='cpu', weights_only=False)
        if not hasattr(model, 'eval'):
            raise ValueError("ì €ì¥ëœ ëª¨ë¸ì´ ì™„ì „í•œ ê°ì²´ê°€ ì•„ë‹™ë‹ˆë‹¤.")
        
        model.eval()
        print(f"âœ… ì™„ì „í•œ ëª¨ë¸ ê°ì²´ ë¡œë“œ: {model_path}")
        print(f"ëª¨ë¸ íƒ€ì…: {type(model)}")
        
        # 2. ìŠ¤ì¼€ì¼ëŸ¬ì™€ í”¼ì²˜ ì •ë³´ ë¡œë“œ
        with open(scaler_path, 'rb') as f:
            scaler_data = pickle.load(f)
        
        scaler = scaler_data['scaler']
        feature_cols = scaler_data['feature_cols']
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ: {scaler_path}")
        print(f"í”¼ì²˜ ê°œìˆ˜: {len(feature_cols)}")
        
        return model, scaler, feature_cols
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None, None

def check_model_exists(save_dir='./models'):
    """í•™ìŠµëœ ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    
    model_path = os.path.join(save_dir, 'transformer_classifier.pt')
    scaler_path = os.path.join(save_dir, 'scaler_and_features.pkl')
    
    if os.path.exists(model_path) and os.path.exists(scaler_path):
        print(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë°œê²¬: {model_path}")
        
        # ëª¨ë¸ íƒ€ì… í™•ì¸ (PyTorch 2.6+ í˜¸í™˜)
        try:
            loaded_model = torch.load(model_path, map_location='cpu', weights_only=False)
            if hasattr(loaded_model, 'eval'):
                print("âœ… ì™„ì „í•œ ê°ì²´ í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìŒ")
                return model_path, scaler_path
            else:
                print("âŒ State dict í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìŒ - ìƒˆë¡œ í•™ìŠµ í•„ìš”")
                return None, None
        except Exception as e:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return None, None
    else:
        print("âŒ ê¸°ì¡´ ëª¨ë¸ ì—†ìŒ. ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
        return None, None

def evaluate_model(model, X_test, y_test, test_df, feature_cols):
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (ì •í™•ë„ë§Œ)"""
    
    if len(X_test) == 0:
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return None
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    model.eval()
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
    with torch.no_grad():
        y_pred_proba = model(X_test_tensor).cpu().numpy().flatten()
    
    y_pred = (y_pred_proba > 0.5).astype(int)
    
    # ì •í™•ë„ë§Œ ê³„ì‚°
    accuracy = accuracy_score(y_test, y_pred)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*50)
    print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
    print("="*50)
    print(f"ì •í™•ë„ (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
    print(f"\ní´ë˜ìŠ¤ë³„ í†µê³„:")
    print(f"ì‹¤ì œ ë¼ë²¨ ë¶„í¬: í•˜ë½={np.sum(y_test==0)}, ìƒìŠ¹={np.sum(y_test==1)}")
    print(f"ì˜ˆì¸¡ ë¼ë²¨ ë¶„í¬: í•˜ë½={np.sum(y_pred==0)}, ìƒìŠ¹={np.sum(y_pred==1)}")
    
    return {
        'accuracy': accuracy,
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }

def simple_backtest(results, test_df, window=30):
    """ê°„ë‹¨í•œ ë°±í…ŒìŠ¤íŠ¸ (ìˆ˜ìµë¥  ê³„ì‚°)"""
    
    if len(test_df) <= window:
        print("âŒ ë°±í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
    
    # ìœˆë„ìš° í¬ê¸°ë§Œí¼ ì¡°ì •ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_subset = test_df.iloc[window:].copy().reset_index(drop=True)
    
    if len(test_subset) != len(results['predictions']):
        print(f"âš ï¸ ë°ì´í„° ê¸¸ì´ ë¶ˆì¼ì¹˜: test_subset={len(test_subset)}, predictions={len(results['predictions'])}")
        return None
    
    # ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
    test_subset['prediction'] = results['predictions']
    test_subset['probability'] = results['probabilities']
    
    # ì‹¤ì œ 5ì¼ í›„ ìˆ˜ìµë¥  ê³„ì‚°
    test_subset = test_subset.sort_values(['ticker', 'date']).reset_index(drop=True)
    test_subset['actual_return'] = test_subset.groupby('ticker')['close'].transform(
        lambda x: (x.shift(-5) / x) - 1
    )
    
    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
    valid_data = test_subset.dropna(subset=['actual_return'])
    
    if len(valid_data) == 0:
        print("âŒ ë°±í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ì˜ˆì¸¡ì— ë”°ë¥¸ ìˆ˜ìµë¥  ê³„ì‚°
    buy_signals = valid_data[valid_data['prediction'] == 1]
    sell_signals = valid_data[valid_data['prediction'] == 0]
    
    if len(buy_signals) == 0:
        print("âŒ ë§¤ìˆ˜ ì‹ í˜¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ì„±ê³¼ ê³„ì‚°
    avg_return_buy = buy_signals['actual_return'].mean()
    avg_return_sell = sell_signals['actual_return'].mean() if len(sell_signals) > 0 else 0
    avg_return_total = valid_data['actual_return'].mean()
    
    # ìŠ¹ë¥  ê³„ì‚°
    buy_win_rate = (buy_signals['actual_return'] > 0).mean()
    total_win_rate = (valid_data['actual_return'] > 0).mean()
    
    print("\n" + "="*50)
    print("ğŸ“ˆ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*50)
    print(f"ë§¤ìˆ˜ ì‹ í˜¸ ê°œìˆ˜: {len(buy_signals)}")
    print(f"ë§¤ë„/ê´€ë§ ì‹ í˜¸ ê°œìˆ˜: {len(sell_signals)}")
    print(f"ë§¤ìˆ˜ ì‹ í˜¸ í‰ê·  ìˆ˜ìµë¥ : {avg_return_buy:.4f} ({avg_return_buy*100:.2f}%)")
    print(f"ë§¤ìˆ˜ ì‹ í˜¸ ìŠ¹ë¥ : {buy_win_rate:.4f} ({buy_win_rate*100:.2f}%)")
    print(f"ì „ì²´ í‰ê·  ìˆ˜ìµë¥ : {avg_return_total:.4f} ({avg_return_total*100:.2f}%)")
    print(f"ì „ì²´ ìŠ¹ë¥ : {total_win_rate:.4f} ({total_win_rate*100:.2f}%)")
    print(f"ì „ëµ ì´ˆê³¼ ìˆ˜ìµë¥ : {avg_return_buy - avg_return_total:.4f} ({(avg_return_buy - avg_return_total)*100:.2f}%)")
    
    return {
        'buy_signals': len(buy_signals),
        'sell_signals': len(sell_signals),
        'avg_return_buy': avg_return_buy,
        'avg_return_total': avg_return_total,
        'buy_win_rate': buy_win_rate,
        'total_win_rate': total_win_rate,
        'excess_return': avg_return_buy - avg_return_total
    }
    
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì‹œì‘...")
    df = load_and_process_data()
    
    print("í•™ìŠµ ë°ì´í„° ì¤€ë¹„...")
    result = prepare_training_data(df)
    
    if result[0] is None:
        print("âŒ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨!")
        return None, None, None
    
    train_df, test_df, feature_cols, scaler = result
    
    # ê¸°ì¡´ ëª¨ë¸ í™•ì¸
    model_path, scaler_path = check_model_exists()
    
    if model_path and scaler_path:
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
        model, scaler, feature_cols = load_trained_model(model_path, scaler_path)
        
        if model is None:
            print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨! ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
            # ìƒˆë¡œ í•™ìŠµ
            print("ì‹œê³„ì—´ ë°ì´í„° ìƒì„±...")
            X_train, y_train = create_sequence(train_df, feature_cols, window=30)
            
            if len(X_train) == 0:
                print("âŒ ì‹œê³„ì—´ ë°ì´í„° ìƒì„± ì‹¤íŒ¨!")
                return None, None, None
            
            print(f"í•™ìŠµ ë°ì´í„° í˜•íƒœ: X_train {X_train.shape}, y_train {y_train.shape}")
            print(f"í”¼ì²˜ ê°œìˆ˜: {len(feature_cols)}")
            
            print("ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
            model = train_model(X_train, y_train, epochs=50)
            
            print("ëª¨ë¸ ì €ì¥...")
            save_model_and_scaler(model, scaler, feature_cols)
        else:
            print("ğŸ”„ ê¸°ì¡´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    else:
        # ìƒˆë¡œ í•™ìŠµ
        print("ì‹œê³„ì—´ ë°ì´í„° ìƒì„±...")
        X_train, y_train = create_sequence(train_df, feature_cols, window=30)
        
        if len(X_train) == 0:
            print("âŒ ì‹œê³„ì—´ ë°ì´í„° ìƒì„± ì‹¤íŒ¨!")
            return None, None, None
        
        print(f"í•™ìŠµ ë°ì´í„° í˜•íƒœ: X_train {X_train.shape}, y_train {y_train.shape}")
        print(f"í”¼ì²˜ ê°œìˆ˜: {len(feature_cols)}")
        
        print("ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        model = train_model(X_train, y_train, epochs=50)
        
        print("ëª¨ë¸ ì €ì¥...")
        save_model_and_scaler(model, scaler, feature_cols)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€
    if len(test_df) > 30:  # ìœˆë„ìš° í¬ê¸°ë³´ë‹¤ í° ê²½ìš°ì—ë§Œ í‰ê°€
        print("\ní…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€...")
        X_test, y_test = create_sequence(test_df, feature_cols, window=30)
        
        if len(X_test) > 0:
            # ì •í™•ë„ í‰ê°€
            results = evaluate_model(model, X_test, y_test, test_df, feature_cols)
            
            if results:
                # ë°±í…ŒìŠ¤íŠ¸
                backtest_results = simple_backtest(results, test_df, window=30)
            
        else:
            print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì„±ëŠ¥ í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    print("âœ… ì™„ë£Œ!")
    
    return model, scaler, feature_cols

if __name__ == "__main__":
    model, scaler, feature_cols = main()